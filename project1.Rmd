---
title: "Modelling Water Flow with Shrinkage Methods"
author: "Rage Against The Machine Learning"
date: "31/01/2021"
output:
  prettydoc::html_pretty:
    theme: hpstr

    highlight: github
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(data.table)
library(naniar)
library(reshape2)
library(ggplot2)
library(ggcorrplot)
library(h2o)
library(dplyr)
library(lubridate)
library(fastDummies)
library(glmnet)
library(leaps)
library(caret)
```

# Introduction

## Water Flow and Water Level

Statistical prediction of water flow (norsk: vannføring) and water level (norsk: vannstand) in rivers is an increasingly important problem. The problem is tightly linked with the prediction of floods. Because of climate change, the occurrence of floods is predicted to increase, and possibly in areas where floods have been historically rare:

[(Norwegian) Klima, nå og i framtiden](https://www.nve.no/klima/klima-na-og-i-framtiden/?ref=mainmenu)

Floods are potentially deadly for both humans and wildlife, and have huge economic consequences each year. Rivers are also an extremely important resource in many countries. In Norway, 90 % of produced electricity comes from hydropower:

[(Norwegian) Kraftproduksjon](https://energifaktanorge.no/norsk-energiforsyning/kraftforsyningen/)

Good statistical models for water discharge and water level are important in order to optimize the production of electricity. The <i>Norwegian Water Resources and Energy Directorate (NVE)</i> has about 600 water level measurement stations all over Norway:

[(Norwegian) Stasjonsnettet](https://www.nve.no/hydrologi/vannstand-og-vannforing/stasjonsnettet/)

Measurements are going as far back as the 1940's. The <i>Norwegian Meteorological Institute (MET)</i> is responsible for the developed weather measurement and forecasting infrastructure in Norway. Many variables obtained by weather measurements, such as temperature, percipitation and snow content are traditionally used in physical models for water discharge and water level. These physical models usually require parameter fitting and/or field experiments in order to yield good predictions. With the wealth of data available, it is worth considering purely data-driven approaches using measurements from NVE and MET to predict water discharge and water level. In this report we will attempt to apply statistical shrinkage models to predict water discharge at Eggafossen in Trøndelag, Norway. NVE was kind enough to give us water measurements and predictions from the model they are currently using, as well as weather data obtained from MET. 

## Eggafossen
Eggafossen is a location along the Gaula river in Trøndelag. Gaula as a whole is approximately 153 kilometers long and drains a watershed of about 3,661 square kilometres. The river runs through several populated areas as well as along the county road fv30, the highway E6 and the Rørosbanen train rail.

```{r pressure, echo=FALSE, fig.cap="Source: Google maps", out.width = '100%'}
knitr::include_graphics("eggafoss.png")
```

In 2011 there was a large flood in Trøndelag, mainly along the upper parts of Gaula. In particular, Ålen kommune, which is one of the largest population centers close to Eggafossen, suffered large damages (bilde fra dagbladet https://www.dagbladet.no/nyheter/enorme-vannmasser-herjer-alen-sentrum/63582581). The Eggafoss station measured a water discharge about 800 000 litres per second, whereas it normally measures about 20 000-30 000. Even though NVE has the responsibility of warning about floods, the 2011 flood was not predicted or warned about by NVE, and precautionary measurements were not taken. NVE stated in their own report on the matter (https://publikasjoner.nve.no/dokument/2011/dokument2011_12.pdf):
"The risk of flood was underestimated because of several factors. The first percipitation predictions were too low. NVE's hydrological models were inadequate for the situation..."
This motivates research on better prediction models.

## The HBV model
The Hydrologiska Byråns Vattenbalansavdelig (HBV) model (https://en.wikipedia.org/wiki/HBV_hydrology_model, https://www.nve.no/hydrologi/analysemetoder-og-modeller/hbv-modellen/?ref=mainmenu) is a physical model designed for simulating river discharge based on an advanced water balance calculation, specifically designed for rivers in Scandinavia.
The HBV model uses several inputs for 
In particular, the parameters needed are daily precipitation, snow content and temperature. 
In order to fit the model one needs
around 9 parameters that are obtained through data fitting and/or field experiments.
The model is somewhat difficult to approach unless one has experience with hydrology, and we will not go into details here.

Because the HBV model most likely requires a data-driven fitting process, it is worth  asking: Is it possible to make comparable predictions to the HBV model 
using a purely data-driven model? The data driven model would have access to the same data
as the HBV model. If a purely data-driven model is shown to be as good or nearly as good as the HBV model, the model can easily be transferred to other measurement stations. Furthermore,
data-driven models can be used for inference in order to assess what actually causes discharge, and
can be used for confidence intervals and uncertainty measurements more easily than a physical model.

# Exploring the Data

The data from various csv-files from NVE is gathered into the file `raw_data_eggafoss.rds`.

```{r}
# Importing data from Eggafoss
eggafoss = readRDS("data/raw_data_eggafoss.rds")

# Transform column names
names(eggafoss)[6:8] <- c("vannføring", "vannstand", "modellertvannføring")
names(eggafoss)
```

The data set consists of 8 features. They are all measured at 12:00 at the date of measurement.

- `dato`: date of measurements [yyyy-mm-dd]
- `nedbør`: rainfall [m]
- `snødekningsgrad`: snow coverage [%]
- `snøvannekvivalent`: snow's water equivalent [m]
- `temperatur`: temperature [°C]
- `vannføring`: water flow in river [m³/s]
- `vannstand`: water level in river [m]
- `modellertvannføring`: HBV's modelled water flow [m³/s]


As discussed above, we will use `vannføring` as the response, while the other features will become the predictors. We begin by gauging basic information about the data.

```{r}
str(eggafoss)
```

```{r}
summary(eggafoss)
```

We notice that `dato` is given in Date-format, while the other feaures are numerical. Most features, like `nedbør`, `snødekningsgrad`, `vannføring`, `vannstand` og `modellertvannføring`, takes on numerical values greater or equal to zero, while `temperatur` can attain negative values as well. `snødekningsgrad` is given in percent, so its range is between 0 and 100. The `dato` column shows that the collection of the data started in 1941 and goes all the up to the end of 2019, which means we have 78 years of daily data for some of our features. There also seems to be a lot of missing values is the dataset, which we will deal with below.

# Preprocessing and Exploratory Data Analysis

Before we can start fitting our data to different models, we have to preprosess the data and do some exploratory data analysis. The goal of this is making sure we have no missing values and that the data is transformed the way we want it, as well as get a initial feel for the features and their correlation with the response and with each other.

## Handeling Missing values

From the summary of the data above, we see that we are missing some values. We make a plot of the missing values to see just how much we are missing.

```{r}
naniar::vis_miss(eggafoss)
```

We see that we are only missing data for a certain range of dates. From 1941 to 1958 we only have data on `vannføring` and `vannstand`, but nothing else. Since there is not much use predicting `vannføring` only from `vannstand` before 1958, we decide to remove these years from the dataset.

```{r}
# Remove data between 1941 and 1958
eggafoss = eggafoss[6120:28764, ]

# Any NA's left?
any(is.na(eggafoss))
```

After removing the missing data we are left with 22645 observations from Eggafoss, which includes daily data from January 1st 1958 to December 31st 2019.

```{r}
dim(eggafoss)
```

```{r}
head(eggafoss)
```

## Examining Correlation

Now that we have removed all the missing values, we start exploring some of the underlying features of the data. We start by looking at the correlation between the covariates and the response.

```{r}
# Calculate the correlation between the features
corr = cor(eggafoss[, 2:8])

# Plot the correlation
ggcorrplot(corr, hc.order = TRUE, type = "lower", lab = TRUE,
   outline.col = "white",
   ggtheme = ggplot2::theme_gray,
   colors = c("#6D9EC1", "white", "#E46726"))
```

We see that the response `vannføring` is mostly corrolated with `modellertvannføring` and `vannstand`. We make a scatterplot of both of them against the response to see in more detail how they affect each other. To avoid to many points at once, we sample 3000 points from our data so that the underlying structure of the data becomes clearer.

```{r, message=FALSE}
# Extract 5000 samples
eggafoss_sample <- eggafoss[sample(nrow(eggafoss), 3000), ]

# Plot vannføring vs. modellertvannføring
ggplot(data = eggafoss_sample, aes(x = modellertvannføring, y = vannføring)) + 
  geom_point() +
  geom_smooth(colour = "orange", method = "lm") +
  labs(title = "Vannføring vs. modellertvannføring")
```

We see that `vannføring` and `modellertvannføring` are highly correlated, which is no big surprise since `modellertvannføring` is in fact the predicted value of `vannføring` from the HBV model. Thus it does not make sense for us to include this feature in our model. We will rather use the results from the HBV model as a baseline for the results from our own model, and see if we can improve on their results.

```{r, message=FALSE}
# Plot vannføring vs. vannstand
ggplot(data = eggafoss_sample, aes(x = vannstand, y = vannføring)) + 
  geom_point() +
  geom_smooth(colour = "orange", method = "lm") +
  labs(title = "Vannføring vs. vannstand")
```

The height of water in a river is intuitively correlated with the amount of water flow in the river. Low water level, low water flow, and vica versa. We see that the relationship is not linear, but looks more like like an exponential or quadratic function. Using `vannstand` to predict `vannføring` is a little bit problematic from a practical point of view. If we want to model the predicted water flow in a river, we most likely would not have any measurements of the water level that day. The other features, like `temperatur` and `snødekningsgrad` could be infered from good weather forcasting models, and thus would not pose the same practical problems. We conclude that when predicting the response, `vannstand` from the same day as the prediction will not be used.

We will also plot some of the covariates which have a high correlation with each other. This type of multicollinearity can cause problems in regular linear regression models, as it increases the standard error of the coefficients and thus some of the covariates can seem unsignificant, even though they are not. For the type of shrinkage methods we will use in this project, this multicollinearity will cause no problem, as the extra regularization term will regulate the size of the coefficients.

```{r, message=FALSE}
# Plot vannstand vs. temperatur
ggplot(data = eggafoss_sample, aes(x = temperatur, y = vannstand)) + 
  geom_point() +
  geom_smooth(colour = "orange", method = "lm") +
  labs(title = "Vannstand vs. temperatur")
```

We see that there is a positive trend between `vannstand` and `temperature`. It is most clear in the temperature interval between -10 and 5. This can indicate that `vannstand` is seasonal-dependend, and that a high water level occurs in the spring.

```{r, message=FALSE}
# Plot temperatur vs. snødekningsgrad
ggplot(data = eggafoss_sample, aes(x = temperatur, y = snødekningsgrad)) + 
  geom_point() +
  geom_smooth(colour = "orange", method = "lm") +
  labs(title = "Snødekningsgrad vs. temperatur")
```

We see that there is a negative correlation between `snødekningsgrad` and `temperature`, which again hints at a seasonal pattern. Below -10°C the snow coverage is at 100%. It starts decreasing once the temperature increases and winter becomes summer.

```{r, message=FALSE}
# Plot snødekningsgrad vs. snøvannekvivalent
ggplot(data = eggafoss_sample, aes(x = snøvannekvivalent, y = snødekningsgrad)) + 
  geom_point() +
  geom_smooth(colour = "orange", method = "lm") +
  labs(title = "Snødekningsgrad vs. snøvannekvivalent")
```

Between `snødekningsgrad` and `snøvannekvivalent` we see that there is somewhat of a linear relationship. The more snow covers the ground, the more water is produced when the show melts. Since `snødekningsgrad` only gives us the percentage of the ground covered by snow, it stops at 100%, and thus will not give us a good indication of how much snow there actually is on the ground. `snøvannekvivalent` can provide a better measure of this. Since the values are so similar, both might not be needed in our model, and might be removed, or set close to zero, by the shrinkage methods.

## Understanding the Seasonality of the Response

One of the features of our original data set is `dato`, the date at which each observation is measured. We cannot use this feature directly in building our model, but based on the analysis above, it might be beneficial to add some information about the time of each measurement, as seasonality is present in the data. To make an even stronger argument of this we look the response `vannføring` as a time series.

```{r, message=FALSE}
# Time series of vannføring
ggplot(data = eggafoss, aes(x = dato, y = vannføring)) +
  geom_line() + 
  geom_smooth(colour = "orange", method = "lm") + 
  labs(title = "Vannføring from 1958 to 2019")
```

From the plot above we see that there is definitely some trends in the data that repeats yearly. We take a closer look by plotting some of the year on top of each other.

```{r}
years = c(1960, 1980, 2000, 2019)

eggafoss %>%
  dplyr::mutate(year = factor(lubridate::year(dato))) %>%
  dplyr::filter(year %in% years) %>%
  ggplot() +
  geom_line(aes(x = lubridate::yday(dato), y = vannføring, group = year, col = year)) +
  ggtitle("Comparison of vannføring in 1960, 1980, 2000 and 2019") + 
  xlab("day")
```

We see that for the different years, a lot of the same trends are prevalent. During the winter month the water flow is usually very low, while during the spring and fall it increases dramatically. We make a boxplot of the `vannføring` for the different months:

```{r}
eggafoss %>%
  ggplot() +
  geom_boxplot(aes(group = lubridate::month(dato), x = lubridate::month(dato), y = vannføring)) + 
  ggtitle("Boxplot of monthly vannføring") + 
  xlab("month")
```

Based on the plot above we make an argument that an interesting covariate to add to our model is `month`, which gives information about which month the observaion is from. We could also have chosen to include season instead of month, but because of the diffuculty to divide the pattern we see into seasons we might loose some information, and instead choose to do the finer monthly division. Adding more variables than necessary should not be a problem in this project, as the regularization methods will remove the variables which are not significant for the prediction of the response.

## Adding information about previous days
We will attempt to model the response as
$$ f_n(t_{k+1}) = \beta_0 + \sum_{i = 1, i \neq n}^N \beta_i f_i(t_{k+1}) + \sum_{i =1}^N\sum_{j = 1}^d \beta_{(N-1)(1+i) + j}f_i(t_{k + 1 - j}),$$
where $f_n(t_{k+1})$ is the value of covariate $n$ at time $t_{k+1}$. The response `vannføring` is a linear model of other covariates on the same day (precipitation, temperature, snow) and the covariates including the response for the previous days. This way our model will capture not only the relevant paramters from the same day (assuming we have a good weather forecast, so precipitation, temperature and snow is known), but relevant parameters from previous days. The motivation behind this approach is that, for rivers, we would expect the water discharge to exhibit some delay in the response of the other covariates. For example, a heavy rainfall might lead to large water discharge after a couple of days as it takes time for the rainfall to travel through the soil into the river. In addition, we would expect a large change in snow (melting) will lead to large water discharge
when this melted water reaches the river.

The function implemented below makes it easy to specify how many years of the data you want to train on and how many of the previous day's data we want to include in the model. It outputs a test and training set, both as a dataframe and as a model matrix for the glmnet methods. We can train on up to 60 years of data, from 1958 to 2018. We always use 2019 as the test set, to see how well our method predicts a whole new year.

We want to emphasize that a linear model might not be the best way to model the response in this example, but as the problem will include a lot of covariates, some of which are highly correlated, it makes a great case study for the shrinkage methods.

```{r}
# Function which does all of the data preparation
# It does not scale or center the variables

data_preparation = function(df, years, days) {
  
  # Only include specified years + 1 year for testing
  df = df %>% dplyr::filter(format(dato, "%Y") > (2019-years-2))
  
  # Which covariates we want to include
  doVannstand = FALSE
  doVannføring = TRUE
  doSnøvannekvivalent = TRUE
  doSnødekningsgrad = TRUE
  doNedbør = TRUE
  doTemperatur = TRUE
  
  sumDo = doVannstand + doSnøvannekvivalent + doSnødekningsgrad +
    doNedbør + doTemperatur + doVannføring
  names = length(names(df))
  
  # Extract information about previous days
  for (i in 1:days){
    len = length(df$dato)
    if (i == 1){
      do = 0
      if (doVannstand) {
        do = do + 1
        data = data.frame(c(NA,df$vannstand[1:(len-1)]))
        df = data.frame(c(df, data))
        name = paste("vannstand",i,"dager",sep="")
        names(df)[names+do] = name
      }
      if (doSnøvannekvivalent) {
        do = do + 1
        data = data.frame(c(NA,df$snøvannekvivalent[1:(len-1)]))
        df = data.frame(c(df, data))
        name = paste("snøvannekvivalent",i,"dager",sep="")
        names(df)[names+do] = name
      }
      if (doSnødekningsgrad) {
        do = do + 1
        data = data.frame(c(NA,df$snødekningsgrad[1:(len-1)]))
        df = data.frame(c(df, data))
        name = paste("snødekningsgrad",i,"dager",sep="")
        names(df)[names + do] = name
      }
      if (doNedbør) {
        do = do + 1
        data = data.frame(c(NA,df$nedbør[1:(len-1)]))
        df = data.frame(c(df, data))
        name = paste("nedbør",i,"dager",sep="")
        names(df)[names + do] = name
      }
      if (doTemperatur) {
        do = do + 1
        data = data.frame(c(NA,df$temperatur[1:(len-1)]))
        df = data.frame(c(df, data))
        name = paste("temperatur",i,"dager",sep="")
        names(df)[names + do] = name
      }
      if (doVannføring) {
        do = do + 1
        data = data.frame(c(NA,df$vannføring[1:(len-1)]))
        df = data.frame(c(df, data))
        name = paste("vannføring",i,"dager",sep="")
        names(df)[names + do] = name
      }
    }
    else {
      do = 0
      if (doVannstand) {
        do = do + 1
        data = data.frame(c(NA,df[1:(len-1), names + (i-2)*sumDo + do]))
        df = data.frame(c(df, data))
        name = paste("vannstand",i,"dager",sep="")
        names(df)[names+(i-1)*sumDo + do] = name
      }
      if (doSnøvannekvivalent) {
        do = do + 1
        data = data.frame(c(NA,df[1:(len-1), names + (i-2)*sumDo + do]))
        df = data.frame(c(df, data))
        name = paste("snøvannekvivalent",i,"dager",sep="")
        names(df)[names+(i-1)*sumDo + do] = name
      }
      if (doSnødekningsgrad) {
        do = do + 1
        data = data.frame(c(NA,df[1:(len-1), names + (i-2)*sumDo + do]))
        df = data.frame(c(df, data))
        name = paste("snødekningsgrad",i,"dager",sep="")
        names(df)[names+(i-1)*sumDo + do] = name
      }
      if (doNedbør) {
        do = do + 1
        data = data.frame(c(NA,df[1:(len-1), names + (i-2)*sumDo + do]))
        df = data.frame(c(df, data))
        name = paste("nedbør",i,"dager",sep="")
        names(df)[names+(i-1)*sumDo + do] = name
      }
      if (doTemperatur) {
        do = do + 1
        data = data.frame(c(NA,df[1:(len-1), names + (i-2)*sumDo + do]))
        df = data.frame(c(df, data))
        name = paste("temperatur",i,"dager",sep="")
        names(df)[names+(i-1)*sumDo + do] = name
      }
      if (doVannføring) {
        do = do + 1
        data = data.frame(c(NA,df[1:(len-1), names + (i-2)*sumDo + do]))
        df = data.frame(c(df, data))
        name = paste("vannføring",i,"dager",sep="")
        names(df)[names+(i-1)*sumDo + do] = name
      }
    }
  }
  
  # Remove the first days of the dataset where we don't have values for all the covariates
  df = na.omit(df)
  
  # Add month variable as factor
  df = df %>%
    dplyr::mutate(df, month = lubridate::month(dato))
  df$month = as.factor(df$month)
  
  # Make dummy variables
  df = fastDummies::dummy_cols(df, select_columns = "month",
                               remove_first_dummy = TRUE,
                               remove_selected_columns = TRUE)
  
  # Split into training and test sets
  train = df %>% dplyr::filter(format(dato, "%Y") < 2019)
  test = df %>% dplyr::filter(format(dato, "%Y") == 2019) 
  
  # Remove the first seven days of the test set, such that we get no leakage from the train set
  test = test[(days + 1):dim(test)[1], ]
  
  # Make data into model matrices
  x_train = model.matrix(vannføring~. -dato -vannstand -modellertvannføring, data = train)[,-1]
  y_train = train$vannføring
  
  x_test = model.matrix(vannføring~ .-dato -vannstand -modellertvannføring, data = test)[,-1]
  y_test = test$vannføring
  
  return(list(df, train, test, x_train, y_train, x_test, y_test))
}
```

# Modelling

We have chosen to use 10 years of data to train our model. The reason for this is that... (explain why we don't use all the data!). We include data from the past 7 days as covariates, such that we might capture how snow melting, temperature change and percipitation can influense the response at a current date. If the later days do not influence the response, we hope that the models will detect this and remove them from the model.

We train on the observation between 2008 and 2018, and test the models on 2019. The dimensions of the test and training set can be seen below. We will try to fit a linear model, a subset model, a lasso model, an elastic net model and a fused lasso model, and see which ones are best at prediction and inference. it is also interesting to see if we can beat the HBV's modelled `vannføring` with our purely data-driven models.

```{r}
years = 10 # max 60
days = 7

data = data_preparation(eggafoss, years, days)
df = data[[1]]
train = data[[2]]
test = data[[3]]
x_train = data[[4]]
y_train = data[[5]]
x_test = data[[6]]
y_test = data[[7]]
```

```{r}
dim(train)
```

```{r}
dim(test)
```

## Linear model

First we start of by fitting a linear model to our data. This is done such that we get an idea of which covariates might be significant to our model and how well a model with all the covariates predicts new observations. We hope that by using model selection and regularization methods we can increase the interpretability, and maybe also the prediction, of the linear model.

```{r}
# Linear model
lm.fit = lm(vannføring~.-dato -vannstand -modellertvannføring, data = train)
summary(lm.fit)
```

We see that only a few of the 54 covariates are significant in the model, and that it explains around 91% of the variability in the data. We cannot completely trust the coeffisient estimates as we know some of the covariates are highly correlated. We make a prediction of the response with the test set, and calculate the MSE of the linear model as a baseline for the other models.

```{r}
lm.pred = predict(lm.fit, test)
mean((test$vannføring - lm.pred)^2)
```

## Linear model with subset selection

We also want to see which variables a subset selection would choose, and if a linear model with these variables would preform better or as good as the linear model with all the covariates. We do both forward and backward selection, and choose the covariates which yields the smallect BIC and Cp values.

```{r}
# Subset model
fsub.fit = regsubsets(vannføring~ .-dato -vannstand -modellertvannføring, nvmax = 60, data = train, method="forward")

par(mfrow=c(2,2))

plot(summary(fsub.fit)$bic, xlab = "Number of Variables", ylab = "BIC", type = 'l', main = "BIC from Forward Selection")
l = which.min(summary(fsub.fit)$bic)
points(l, summary(fsub.fit)$bic[l], col="red", cex=2, pch=20)

plot(summary(fsub.fit)$cp, xlab = "Number of Variables", ylab = "Cp", type = 'l', main = "Cp from Forward Selection")
l = which.min(summary(fsub.fit)$cp)
points(l, summary(fsub.fit)$cp[l], col="red", cex=2, pch=20)

bsub.fit = regsubsets(vannføring~ .-dato -vannstand -modellertvannføring, nvmax = 60, data = train, method="backward")

plot(summary(bsub.fit)$bic, xlab = "Number of Variables", ylab = "BIC", type = 'l', main = "BIC from Backward Selection")
l = which.min(summary(bsub.fit)$bic)
points(l, summary(bsub.fit)$bic[l], col="red", cex=2, pch=20)

plot(summary(bsub.fit)$cp, xlab = "Number of Variables", ylab = "Cp", type = 'l', main = "Cp from Backward Selection")
l = which.min(summary(bsub.fit)$cp)
points(l, summary(bsub.fit)$cp[l], col="red", cex=2, pch=20)
```

We choose the covariates with the lowest BIC values from the backward selection.

```{r}
best = which.min(summary(bsub.fit)$bic)
coeffs = names(coef(bsub.fit, best))[2:length(coef(bsub.fit, best))]
train_subset = subset(train, select = c(coeffs))
train_subset["vannføring"] = train$vannføring

bestsub.fit = lm(vannføring~., data = train_subset)
summary(bestsub.fit)
```

We see that by using backward selection, the optimal model has 13 variables, which are all significant in the model. We observe that the temperature, amount of snow and amount of percipitation on the day of measurement is important, as well as the response value, precipitation, snow amount and temperature from some of the past days. Interestingly, the model does not care about the temperature at day 2, 3 or 4, but the temperature at day 5 becomes significant. The same can be seen for some of the other covariates. This could just be our model fitting noise in the data, but might also point to a delay in the covariates impact on the response, which the model manages to catch. Using the model on the test set yields the following MSE:

```{r}
bestsub.pred = predict(bestsub.fit, test)
mean((test$vannføring - bestsub.pred)^2)
```

We get a MSE very similar to that of the linear model, which means that the prediction power of the model has not gone down when we have removed some of the non-significant variables.

## Lasso model

Now that we have seen that a subset of the covariates can explain the variance in the data as well, we want to try some regularization methods on the data. The lasso is a shrinkage method which utilizes a $L_1$ penalty to shrink and remove some of the covariates in the model. The lasso coefficients are given by

$$ \hat{\beta}^{lasso} = \arg\min_{\beta} \left \{ \frac{1}{2}\sum_{i=1}^N (y_i - \beta_0 -\sum_{j=1}^p x_{ij}\beta_j)^2 + \lambda \sum_{j=1}^p |\beta_j| \right \}$$

To set the tuning parameter $\lambda$, we use 10 fold cross validation. Below is a plot of the...

Reason why lasso is bad: since we train on so much data, the model does not overfit. Choosing the 1sd lambda thus leads to underfitting, and we see a bad prediction rate. If we choose the minimum lambda we will have a lot of predictors, but the prediction will be better.

```{r}
start = glmnet(x = x_train, y = y_train, alpha = 1, standardize = TRUE)
autolambda = start$lambda
lambdagrid = c(autolambda, 0.5, 0.3 ,0.2 ,0.1)

lasso.fit = glmnet(x = x_train, y = y_train, alpha = 1, standardize = TRUE, lambda = lambdagrid)
cvlasso.fit = cv.glmnet(x_train, y_train, alpha = 1, standardize = TRUE, lambda = lambdagrid)

plot(cvlasso.fit)
```

```{r}
plot(lasso.fit, xvar = "lambda", label = TRUE)
abline(v=log(cvlasso.fit$lambda.1se))
```

```{r}
cvlasso.pred = predict(cvlasso.fit, s = cvlasso.fit$lambda.1se, newx = x_test)
mean((test$vannføring - cvlasso.pred)^2)
```

## Elastic net model

$$ \hat{\beta}^{elastic \ net} = \arg\min_{\beta} \left \{ \frac{1}{2}\sum_{i=1}^N (y_i - \beta_0 -\sum_{j=1}^p x_{ij}\beta_j)^2 + \lambda \sum_{j=1}^p (\alpha \beta_j^2 + (1-\alpha)|\beta_j|) \right \}$$

```{r}
control <- trainControl(method = "repeatedcv", 
                              number = 5, 
                              repeats = 5, 
                              search = "random")

elastic_model <- train(vannføring~.-dato -vannstand -modellertvannføring, 
                       data = train, 
                       method = "glmnet", 
                       preProcess = c("center", "scale"), 
                       tuneLength = 25, 
                       trControl = control) 

elastic_model
```

```{r}
elastic.fit = glmnet(x = x_train, y = y_train, alpha = elastic_model$bestTune$alpha, standardize = TRUE, lambda = elastic_model$bestTune$lambda)

cvelastic.pred = predict(elastic.fit, s = elastic_model$bestTune$lambda, newx = x_test)
mean((test$vannføring - cvelastic.pred)^2)
```

## Fused lasso model

## Comparison of the coefficients

```{r}
resmat = cbind(coef(elastic.fit, s = elastic_model$bestTune$lambda), coef(lasso.fit, s = cvlasso.fit$lambda.1se), coef(lm.fit))
colnames(resmat)=c("elastic net", "lasso","lm")
print(resmat)
```

```{r}
coef(bestsub.fit)
```

# Prediction

Vi burde finne en bedre visualisering enn denne, eller lage flere.

```{r}
ggplot(test, aes(x = dato)) +
  geom_line(aes(y = vannføring)) +
  geom_line(aes(y = modellertvannføring), color = "blue", linetype = "twodash") +
  geom_line(aes(y = lm.pred), color = "red", linetype = "twodash") +
  geom_line(aes(y = bestsub.pred), color = "orange", linetype = "twodash") +
  geom_line(aes(y = cvlasso.pred), color = "green", linetype = "twodash") +
  geom_line(aes(y = cvelastic.pred), color = "yellow", linetype = "twodash")
```

```{r}
mean((test$vannføring - test$modellertvannføring)^2)
mean((test$vannføring - lm.pred)^2)
mean((test$vannføring - bestsub.pred)^2)
mean((test$vannføring - cvlasso.pred)^2)
mean((test$vannføring - cvelastic.pred)^2)
```

# Inference
